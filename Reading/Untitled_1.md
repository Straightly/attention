# Untitled

Yes â€” youâ€™re wise to raise that concern.

What youâ€™re describing could absolutely raise red flags, depending on how your AI app is framed, used, and integrated into clinical decision-making.

Letâ€™s unpack both the regulatory and ethical implications â€” and then Iâ€™ll show how to build it safely.

# **ğŸ” Your Appâ€™s Intended Function:**

# ğŸ§  AI to assist providers in checking for â€œgapsâ€ or evidence that may refute or support a diagnosis â€” post-diagnosis â€” by scanning the patientâ€™s records.

This can be framed in two very different ways:

| **Perspective** | **Framing** | **Risk Level** |
| --- | --- | --- |
| âœ… Clinical validation | â€œHelp clinicians catch mistakes or oversightsâ€ | Low |
| âš ï¸ Revenue optimization | â€œHelp increase risk scores or identify undocumented diagnosesâ€ | High |

# **âš ï¸ Why It Could Be Seen as**

# **Upcoding Pressure**

# **:**

If your app:

- Suggests adding diagnoses with higher Hierarchical Condition Category (HCC) weights
- Focuses on capturing more billable codes
- Is used after the fact, not during clinical care
- Is funded or directed by payers or revenue teams, not clinical teams

â€¦then yes, it could be construed as collusion or pressure to upcode, particularly under:

- DOJ scrutiny of Medicare Advantage fraud
- False Claims Act implications
- CMS RADV (Risk Adjustment Data Validation) audits

# **ğŸ›¡ï¸ How to Make It Legally & Ethically Safe**

To reduce risk while preserving functionality:

**âœ…**

**Frame it as a Clinical Safety/QA Tool**

- â€œHelps providers avoid underdiagnosis or incorrect documentationâ€
- Emphasize medical appropriateness, not revenue

**âœ…**

**Include Refutation and Confirmation**

- Donâ€™t just suggest what to add â€” also highlight where existing diagnoses may not be supported
- This is key: a balanced tool is more defensible

**âœ…**

**Use Transparent Logic**

- Document what evidence the AI uses (e.g., lab value X + note Y = suggest Z)
- Let the provider decide; donâ€™t auto-populate diagnoses

**âœ…**

**Auditability**

- Keep a log of all AI suggestions vs. final provider decisions
- Let compliance/auditors track how AI impacted outcomes

**âœ…**

**Integrate with Clinical Workflow**

- Embed in EHR or at the point of care, not in billing backend
- Have it used by clinicians, not by coders or risk teams alone

# **ğŸ“˜ Example of Safe Messaging:**

# â€œAI Assistant to help validate or refine clinical diagnoses based on full patient history, reducing both overdiagnosis and underdiagnosis for improved quality of care.â€

vs.

# âŒ â€œAI tool to ensure maximum risk-adjusted reimbursement from Medicare Advantage plans.â€

# **ğŸ§  Final Word:**

Youâ€™re smart to ask early. The same app could either:

- Be seen as a quality tool, if developed with clinical integrity
- Or as a fraud accelerator, if tied to revenue-focused usage without safeguards

If youâ€™d like, I can help sketch out a compliance-friendly design doc or a or a terms-of-use disclaimer tailored for provider-facing AI tools.